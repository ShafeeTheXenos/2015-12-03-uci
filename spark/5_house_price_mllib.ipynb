{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_RDD = sc.textFile(\"/data/houses.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapper_parse_lines(line):\n",
    "    \"\"\"Parse line into (neighborhoood, price) pair\"\"\"\n",
    "    words = line.split()\n",
    "    return (words[1], float(words[2]), int(words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_prices_RDD = text_RDD.map(mapper_parse_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Downtown', 400000.0, 3),\n",
       " (u'Downtown', 240000.0, 2),\n",
       " (u'Hilltop', 650000.0, 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_prices_df = sqlContext.createDataFrame(house_prices_RDD, [\"neighborhood\", \"price\", \"bedrooms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------+\n",
      "|neighborhood|   price|bedrooms|\n",
      "+------------+--------+--------+\n",
      "|    Downtown|400000.0|       3|\n",
      "|    Downtown|240000.0|       2|\n",
      "|     Hilltop|650000.0|       3|\n",
      "+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_features(row):\n",
    "    return Row(neighborhood=row.neighborhood,\n",
    "               features=Vectors.dense([row.bedrooms, row.price]))\n",
    "    \n",
    "house_prices_features = sqlContext.createDataFrame(\n",
    "    house_prices_df.map(create_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|      features|neighborhood|\n",
      "+--------------+------------+\n",
      "|[3.0,400000.0]|    Downtown|\n",
      "|[2.0,240000.0]|    Downtown|\n",
      "|[3.0,650000.0]|     Hilltop|\n",
      "+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name (default: features)\n",
      "initMode: the initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: steps for k-means initialization mode (default: 5)\n",
      "k: number of clusters to create (default: 2)\n",
      "maxIter: max number of iterations (>= 0) (default: 20)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: -7649703878154674547)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = kmeans.fit(house_prices_features)\n",
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  2.50000000e+00,   3.20000000e+05]),\n",
       " array([  3.00000000e+00,   6.50000000e+05])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed = model.transform(house_prices_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([3.0, 400000.0]), neighborhood=u'Downtown', prediction=0),\n",
       " Row(features=DenseVector([2.0, 240000.0]), neighborhood=u'Downtown', prediction=0),\n",
       " Row(features=DenseVector([3.0, 650000.0]), neighborhood=u'Hilltop', prediction=1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_houses = sqlContext.createDataFrame([\n",
    "    (Vectors.dense([3.0, 450000]),),\n",
    "    (Vectors.dense([2.0, 500000]),),        \n",
    "        ],\n",
    "    [\"features\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      features|\n",
      "+--------------+\n",
      "|[3.0,450000.0]|\n",
      "|[2.0,500000.0]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_houses.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([3.0, 450000.0]), prediction=0),\n",
       " Row(features=DenseVector([2.0, 500000.0]), prediction=1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(new_houses).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"neighborhood\",\n",
    "                              outputCol=\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_prices_features_labels = stringIndexer.fit(house_prices_features).transform(house_prices_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([3.0, 400000.0]), neighborhood=u'Downtown', label=0.0),\n",
       " Row(features=DenseVector([2.0, 240000.0]), neighborhood=u'Downtown', label=0.0),\n",
       " Row(features=DenseVector([3.0, 650000.0]), neighborhood=u'Hilltop', label=1.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_features_labels.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name (default: label)\n",
      "maxIter: max number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.1, current: 0.01)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 1e-06)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\n",
    "\n",
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(house_prices_features_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([3.0, 400000.0]), neighborhood=u'Downtown', rawPrediction=DenseVector([2.9666, -2.9666]), probability=DenseVector([0.951, 0.049]), prediction=0.0),\n",
       " Row(features=DenseVector([2.0, 240000.0]), neighborhood=u'Downtown', rawPrediction=DenseVector([3.9863, -3.9863]), probability=DenseVector([0.9818, 0.0182]), prediction=0.0),\n",
       " Row(features=DenseVector([3.0, 650000.0]), neighborhood=u'Hilltop', rawPrediction=DenseVector([-1.9976, 1.9976]), probability=DenseVector([0.1195, 0.8805]), prediction=1.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.transform(house_prices_features).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([3.0, 450000.0]), rawPrediction=DenseVector([1.9737, -1.9737]), probability=DenseVector([0.878, 0.122]), prediction=0.0),\n",
       " Row(features=DenseVector([2.0, 500000.0]), rawPrediction=DenseVector([-1.1764, 1.1764]), probability=DenseVector([0.2357, 0.7643]), prediction=1.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.transform(new_houses).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
